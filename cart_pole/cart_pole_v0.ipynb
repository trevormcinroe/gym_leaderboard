{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQLearning:\n",
    "    \n",
    "    \"\"\"\n",
    "    Q(S,A)←Q(S,A) +α[R+γmaxaQ(S′,a)−Q(S,A)]\n",
    "\n",
    "    Algorithm parameters:  step size α∈(0,1], small ε >0\n",
    "    Initialize Q(s,a), for all s∈S+, a∈A(s), arbitrarily except that Q(terminal,·) = 0\n",
    "    Loop for each episode:\n",
    "        Initialize S \n",
    "        Loop for each step of episode:\n",
    "            Choose A from S using policy derived from Q(e.g.,ε-greedy)\n",
    "            Take action A, observe R, S′\n",
    "            Q(S,A)←Q(S,A) +α[R+γmaxaQ(S′,a)−Q(S,A)]\n",
    "            S←S′; A←A′;\n",
    "        until S is terminal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 buckets,\n",
    "                 decay,\n",
    "                 min_epsilon,\n",
    "                 min_alpha,\n",
    "                 gamma,\n",
    "                 environment,\n",
    "                 min_velocity,\n",
    "                 max_velocity):\n",
    "        \n",
    "        self.buckets = buckets\n",
    "        self.decay = decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.min_alpha = min_alpha\n",
    "        self.gamma = gamma\n",
    "        self.environment = environment\n",
    "        \n",
    "        self.lower_bounds = [\n",
    "            self.environment.observation_space.low[0],\n",
    "            min_velocity,\n",
    "            self.environment.observation_space.low[2],\n",
    "            -math.radians(50)\n",
    "        ]\n",
    "        self.upper_bounds = [\n",
    "            self.environment.observation_space.high[0],\n",
    "            max_velocity,\n",
    "            self.environment.observation_space.high[2],\n",
    "            math.radians(50)\n",
    "        ]\n",
    "        \n",
    "        self.action_value_table = None\n",
    "        self.alpha = None\n",
    "        self.epsilon = None\n",
    "        self.episode_cnt = 0\n",
    "        \n",
    "        self.training_reward_history = []\n",
    "        \n",
    "        self.reset_action_value_table()\n",
    "    \n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, min(1., 1. - math.log10((self.episode_cnt + 1) / self.decay)))\n",
    "    \n",
    "    def update_alpha(self):\n",
    "        \"\"\"\"\"\"\n",
    "        self.alpha = max(self.min_alpha, min(1., 1. - math.log10((self.episode_cnt + 1) / self.decay)))\n",
    "    \n",
    "    def reset_action_value_table(self):\n",
    "        \"\"\"Subroutine to reset the values in the action value table\"\"\"\n",
    "        self.action_value_table = np.zeros(self.buckets + (self.environment.action_space.n, ))\n",
    "    \n",
    "    \n",
    "    def state_to_tiles(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (array): an array representing the state. returned from self.environment.step(action)\n",
    "            \n",
    "        Returns:\n",
    "            tuple of len 4 to help navigate through our action value table\n",
    "        \"\"\"\n",
    "        tiles = list()\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            scaling = (s[i] + abs(self.lower_bounds[i])) / (self.upper_bounds[i] - self.lower_bounds[i])\n",
    "            scaled_s = int(round((self.buckets[i] - 1) * scaling))\n",
    "            scaled_s = min(self.buckets[i] - 1, max(0, scaled_s))\n",
    "            tiles.append(scaled_s)\n",
    "        return tuple(tiles)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        # Allow for some exploration...\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return self.environment.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.action_value_table[state])\n",
    "        \n",
    "    def q_learning_update(self, s, a, r, s_prime, a_prime):\n",
    "        \"\"\"\"\"\"\n",
    "        self.action_value_table[s][a] += self.alpha * (r + self.gamma * max(self.action_value_table[s_prime]) \n",
    "                                                       - self.action_value_table[s][a])\n",
    "        \n",
    "    def train(self, n_episodes, reset):\n",
    "        \"\"\"\"\"\"\n",
    "        if reset:\n",
    "            self.reset_action_value_table()\n",
    "            self.episode_cnt = 0\n",
    "            \n",
    "        for _ in range(n_episodes):\n",
    "        \n",
    "            # Resetting the environment and our episode counter\n",
    "            s = self.state_to_tiles(self.environment.reset())\n",
    "\n",
    "            # Resetting our alpha and epsilon\n",
    "            self.update_epsilon()\n",
    "            self.update_alpha()\n",
    "            \n",
    "            terminal_state = False\n",
    "            \n",
    "            episode_cumsum_r = 0\n",
    "            \n",
    "            while not terminal_state:\n",
    "                a = self.choose_action(state=s)\n",
    "                s_prime, r, terminal_state, _ = self.environment.step(action=a)\n",
    "                episode_cumsum_r += r\n",
    "                s_prime = self.state_to_tiles(s=s_prime)\n",
    "                a_prime = self.choose_action(state=s_prime)\n",
    "                self.q_learning_update(s=s, a=a, r=r, s_prime=s_prime, a_prime=a_prime)\n",
    "                s = s_prime\n",
    "                \n",
    "            self.training_reward_history.append(episode_cumsum_r)\n",
    "                \n",
    "            # Updating our episode counter\n",
    "            self.episode_cnt += 1\n",
    "            \n",
    "    def test(self, display):\n",
    "        \"\"\"\"\"\" \n",
    "        s = self.state_to_tiles(s=self.environment.reset())\n",
    "        steps = 0\n",
    "        terminal_state = False\n",
    "        \n",
    "        # We don't need to explore during testing - do we?\n",
    "        self.epsilon = 0\n",
    "        \n",
    "        while not terminal_state:\n",
    "            if display:\n",
    "                self.environment.render()\n",
    "                \n",
    "            a = self.choose_action(state=s)\n",
    "            s_prime, r, terminal_state, _ = self.environment.step(action=a)\n",
    "            s_prime = self.state_to_tiles(s=s_prime)\n",
    "            s = s_prime\n",
    "            steps += 1\n",
    "            \n",
    "        if display:\n",
    "            self.environment.close()\n",
    "            \n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(860720)\n",
    "\n",
    "cart_pole_qlearning = CartPoleQLearning(buckets=(1, 1, 6, 12),\n",
    "                      decay=25,\n",
    "                      min_epsilon=0.1,\n",
    "                      min_alpha=0.1,\n",
    "                      gamma=0.98,\n",
    "                      environment=gym.make('CartPole-v0'),\n",
    "                      min_velocity=-0.5,\n",
    "                      max_velocity=0.5)\n",
    "\n",
    "cart_pole_qlearning.train(n_episodes=130, reset=True)\n",
    "\n",
    "cart_pole_qlearning.test(display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for _ in range(100):\n",
    "    results.append(cart_pole_qlearning.test(display=False))\n",
    "    \n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
